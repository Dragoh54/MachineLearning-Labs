{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338d3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numdifftools as nd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846de5f5",
   "metadata": {},
   "source": [
    "# <font color = 'red'> ЛР 2. Дифференцирование функций многих переменных. Линейная регрессия по произвольному базису. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be203bc0",
   "metadata": {},
   "source": [
    "Сложность: <font color = 'orange'> Нормально  </font>.\n",
    "\n",
    "Дата составления: 16.09.2024\n",
    "\n",
    "Срок выполнения: 2 недели.\n",
    "\n",
    "Автор: ст. преподаватель Кушнеров А.В."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f72d2",
   "metadata": {},
   "source": [
    "## <font color = 'green'> 1. Дифференцирование функции векторного аргумента. База.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17525728",
   "metadata": {},
   "source": [
    "Вспоминаем понятие дифференцирования.\n",
    "\n",
    "Пусть, для начала, $f$ - функция одного аргумента, возвращающая один аргумент. Иными словами, $f:\\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Говорят, что $f$ **дифференцируема** в точке $x_{0}$, если существует конечный предел: $$\\lim\\limits_{h \\to 0} \\frac{f(x_{0}+h) - f(x_{0})}{h} = f'(x_{0})$$.\n",
    "\n",
    "Его называют производной функции в точке $x_{0}$.  Это жу информацию можно записать в дифференциальной форме:  $$f(x_{0}+h) - f(x_{0}) = f'(x_{0}) h + o(h) = [Df_{x_{0}}](h) + o(h)$$.Величину $f'(x_{0}) h = [Df_{x_{0}}](h)$ называют **дифференциалом функции**.\n",
    "\n",
    "\n",
    "Теперь предположим, что функция $f$ работает несколько иначе. Теперь она принимает на вход векторный аргумент, а возвращает всё ещё скалярный $f:\\mathbb{R^{m}} \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "Теперь понятие дифференциала вводят несколько иначе. Мы всё также можем записать: $$f(\\overline{x_{0}}+\\overline{h}) - f(\\overline{x_{0}}) = \\sum\\limits_{j=1}^m \\frac{\\partial f}{\\partial x_{m}}\\bigg\\rvert_{x=\\overline{x_{0}}}h_{i} = [Df_{x_{0}}](\\overline{h}) + o(||\\overline{h}||)$$.\n",
    "\n",
    "Или же в более сжатой форме: $$f(\\overline{x_{0}}+\\overline{h}) - f(\\overline{x_{0}}) = (\\nabla_{x_{0}} f)\\cdot\\overline{h} + o(||\\overline{h}||) $$\n",
    "\n",
    "Под производной в этом случае понимают **вектор** (чаще его пищут в виде вектор-столбца) градиента $\\nabla_{x_{0}} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{1}}\\\\ \\frac{\\partial f}{\\partial x_{2}}\\\\...\\\\\\frac{\\partial f}{\\partial x_{m}} \\end{bmatrix}\\bigg\\rvert_{x=\\overline{x_{0}}}$.\n",
    "\n",
    "В МО мы вынуждены работать с задачами оптимизации как раз таких функций, что немедленно отсылает нас к производным. В частности, используя лекционные записи, вы легко можете получить две несложных формулы матричного дифференцирования.\n",
    "\n",
    "1. Если функция $f(\\overline{x}) = \\overline{a}^{T} \\cdot\\overline{x}$, то $f'(\\overline{x}) = \\overline{a} $. Где $\\overline{x},\\overline{a} $ - вектор столбцы.\n",
    "\n",
    "2.  Если функция $f(\\overline{x}) = \\overline{x}^{T} A\\overline{x}$, то $f'(\\overline{x}) = (A+A^{T})\\overline{x} $. Где $\\overline{x}$ - вектор столбец, $A$ - квадратная матрица соответсвующего размера.\n",
    "\n",
    "\n",
    "В будущем, возможно, мы дополним список. А пока можете поупражняться [самостоятельно](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf). \n",
    "\n",
    "\n",
    "Теперь проверим справедливость формулы 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "034be24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 2, 8, 5, 4, 4, 7, 9, 9, 8],\n",
       "       [7, 9, 3, 8, 9, 2, 5, 7, 1, 8],\n",
       "       [8, 5, 9, 5, 7, 0, 6, 3, 6, 2],\n",
       "       [4, 9, 1, 3, 6, 8, 7, 2, 9, 1],\n",
       "       [7, 5, 9, 2, 0, 9, 2, 8, 9, 6],\n",
       "       [7, 6, 6, 1, 8, 2, 6, 8, 4, 9],\n",
       "       [1, 0, 8, 6, 9, 1, 2, 4, 3, 2],\n",
       "       [9, 4, 9, 1, 1, 8, 5, 0, 5, 0],\n",
       "       [4, 5, 3, 3, 1, 2, 6, 8, 9, 3],\n",
       "       [8, 1, 4, 3, 7, 5, 4, 0, 6, 6]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.randint(10,size =(10,10))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3240e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x): \n",
    "    return np.dot(np.dot(x.T,A),x) #задаём вектор-функцию согласно формуле 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64df4c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.arange(0,10) # конкретные значения x0\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05f341d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([589., 417., 450., 355., 481., 463., 364., 318., 500., 371.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = nd.Gradient(func)(x0) #находим производную в точке x0(градиент) встроенными численными методами\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "860b66cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([589, 417, 450, 355, 481, 463, 364, 318, 500, 371])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A + A.T,x0) # а теперь по нашей формуле. Всё сходится."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a43162",
   "metadata": {},
   "source": [
    "<font color = 'red' size = 5>Задание 1 </font>\n",
    "\n",
    "1. Изучите подробно формулы полученные выше. Попрактикуйтесь самостоятельно в их выводе.\n",
    "2. Получите ещё 2 любых формулы для подобного матричного или векторного дифференцирования. Вывод формул кратко оформите в документе.\n",
    "3. Проверьте справедливость полученных вами формул с помощью встроенных функций пакета numdifftools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "267301a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(a\u001b[38;5;241m.\u001b[39mT,x)\n\u001b[0;32m      7\u001b[0m x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numdifftools\\core.py:490\u001b[0m, in \u001b[0;36mGradient.__call__\u001b[1;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m--> 490\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_output:\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(), result[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numdifftools\\core.py:431\u001b[0m, in \u001b[0;36mJacobian.__call__\u001b[1;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJacobian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numdifftools\\core.py:288\u001b[0m, in \u001b[0;36mDerivative.__call__\u001b[1;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[0;32m    286\u001b[0m x_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 288\u001b[0m     results, f_xi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m     derivative, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extrapolate(\u001b[38;5;241m*\u001b[39mresults)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_output:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numdifftools\\core.py:428\u001b[0m, in \u001b[0;36mJacobian._derivative_nonzero_order\u001b[1;34m(self, x_i, args, kwds)\u001b[0m\n\u001b[0;32m    425\u001b[0m steps2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_steps(steps, x_i, fxi)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_richardson_rule(step_ratio, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrichardson_terms)\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfd_rule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_ratio\u001b[49m\u001b[43m)\u001b[49m, fxi\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numdifftools\\finite_difference.py:583\u001b[0m, in \u001b[0;36mLogRule.apply\u001b[1;34m(self, sequence, steps, step_ratio)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequence, steps, step_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m):\n\u001b[0;32m    572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m    Apply finite difference rule along the first axis.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    581\u001b[0m \n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     f_del, h, original_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m     der_init, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(f_del, h, step_ratio)\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m der_init, h, original_shape\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numdifftools\\finite_difference.py:684\u001b[0m, in \u001b[0;36mLogJacobianRule._vstack\u001b[1;34m(self, sequence, steps)\u001b[0m\n\u001b[0;32m    681\u001b[0m     original_shape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m original_shape[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    683\u001b[0m     f_del \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([np\u001b[38;5;241m.\u001b[39matleast_1d(r)\u001b[38;5;241m.\u001b[39mtranspose(axes)\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sequence])\n\u001b[1;32m--> 684\u001b[0m     h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m steps])\n\u001b[0;32m    685\u001b[0m _assert(f_del\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m h\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun did not return data of correct \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize (it must be vectorized)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f_del, h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atleast_2d(original_shape, ndim)\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "# task2\n",
    "a = np.random.randint(10,size =(10,1))\n",
    "\n",
    "def func(x): \n",
    "    return np.dot(a.T,x)\n",
    "\n",
    "x0 = np.arange(0,10)\n",
    "\n",
    "grad = nd.Gradient(func)(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5666154-ed48-4de6-85ae-70fcf7794e89",
   "metadata": {},
   "source": [
    "## <font color = 'green'> 2. Линейная регрессия по произвольному базису. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96f828-03b7-4709-8f48-5712d9d1c4b7",
   "metadata": {},
   "source": [
    "Пусть задано множество пар признаков(фич) и меток $A = \\{(\\overline{X}_{1},y_{1}),(\\overline{X}_{2},y_{2}),...,(\\overline{X}_{n},y_{n})\\}$. Важно понимать, что теперь каждый элемент множества признаков это вектор состоящий из некоторого числа $k$ признаков  $\\overline{X}_{i} = (x_{1},x_{2},...x_{k})$.\n",
    "\n",
    "Стоит задача получить функцию, позволяющую предсказывать непрерывную метку по $y$ набору признаков $X$. Такую задачу называют множественной регрессией. \n",
    "\n",
    "Для решения данной задачи можно использовать формулу множественной регрессии:$$f(\\overline{x})=\\sum\\limits_{j=1}^m w_{j} \\phi_{j}(\\overline{x}) $$.\n",
    "\n",
    "В приведённой выше формуле, стоит взвешенная сумма некоторых произвольных функций от вектора фич. Это и есть **формула множественной линейной регрессии по произвольному базису**. \n",
    "\n",
    "Частным случаем такой регрессии можно считать тривиальную линейную регрессию, когда $\\phi_{i}(\\overline{X}) = x_{i}$. Тогда добавив фиктивную функцию $\\phi_{0}(\\overline{x}) = 1$ получим формулу **классической линейной регрессии**.\n",
    "\n",
    "$$f(\\overline{x})=w_{0} + \\sum\\limits_{j=1}^k w_{j} x_{j} $$\n",
    "\n",
    "Тут $m = k + 1 $\n",
    "\n",
    "Обратите внимание, что формула парной регресии $f(x) = a x +b $ - тоже частный случай вышеприведённой. \n",
    "\n",
    "Также стоит заметить, *что количество базисных функций может быть как меньшим, так и большим, чем количество изначальных фич*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3bc89-063a-4b74-99bc-652139ac71aa",
   "metadata": {},
   "source": [
    "Теперь приступим к **обучению полученной модели** $f(\\overline{x})=\\sum\\limits_{j=1}^m w_{j} \\phi_{j}(\\overline{x}) $. Оно сводится к нахождению весов $w_{1},...,w_{m}$ на основе исходных обучающих данных.\n",
    "\n",
    "Аналогично парной регрессии составим функцию потерь и минимизируем её: $$ \\mathcal{L}(\\overline{w}) =\\sum\\limits_{i=1}^n (y_{i} - \\sum\\limits_{j=1}^m w_{j} \\phi_{j}(\\overline{X_{i}}))^2 \\rightarrow min$$.\n",
    "\n",
    "Конечно проще записать эту формулу в матричном виде (получите её самостоятельно): $$ \\mathcal{L}(\\overline{w}) = (\\overline{y} - Q\\cdot\\overline{w})^{T}(\\overline{y} - Q\\cdot\\overline{w}), $$\n",
    "где $Q$ - *информационная матрица*, которая определяется следующим образом: $$Q=\\begin{bmatrix}\n",
    "    \\phi_{1}(\\overline{x_{1}})       &  \\phi_{2}(\\overline{x_{1}}) & \\phi_{3}(\\overline{x_{1}}) & \\dots & \\phi_{m}(\\overline{x_{1}}) \\\\\n",
    "    \\phi_{1}(\\overline{x_{2}})       &  \\phi_{2}(\\overline{x_{2}}) & \\phi_{3}(\\overline{x_{2}}) & \\dots & \\phi_{m}(\\overline{x_{2}})   \\\\\n",
    "                            ...\\\\\n",
    "    \\phi_{1}(\\overline{x_{n}})       &  \\phi_{2}(\\overline{x_{n}}) & \\phi_{3}(\\overline{x_{n}}) & \\dots & \\phi_{m}(\\overline{x_{n}}) \n",
    "    \\end{bmatrix}, $$\n",
    "\n",
    "$\\overline{w}$ - вектор столбец весов: $$ \\overline{w} = \\begin{bmatrix} \n",
    "    w_{1}\\\\\n",
    "    w_{2}\\\\\n",
    "    ...\\\\\n",
    "    w_{m}\n",
    "    \\end{bmatrix},\n",
    "    $$\n",
    "\n",
    "$\\overline{y}$ - вектор столбец известных целевых меток: $$ \\overline{y} = \\begin{bmatrix} \n",
    "    y_{1}\\\\\n",
    "    y_{2}\\\\\n",
    "    ...\\\\\n",
    "    y_{n}\n",
    "    \\end{bmatrix}.\n",
    "    $$\n",
    "\n",
    "\n",
    "Далее решаем задачу оптимизации. Отыщем производную полученной скалярной вектор-функции и приравняем к 0, опираясь на знания из п. 1.$$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\nabla \\mathcal{L} =\\nabla \\mathcal{L}((\\overline{y} - Q\\cdot\\overline{w})(\\overline{y} - Q\\cdot\\overline{w})^{T}) = \\nabla \\mathcal{L} (\\overline{y}^{T} \\overline{y}-2\\overline{y}^{T} Q\\overline{w}+\\overline{w}^{T} Q^{T} Q\\overline{w} ) = 0$$\n",
    "\n",
    "Далее применяем формулы из п. 1. $$\\nabla \\mathcal{L} (\\overline{y}^{T} \\overline{y}-2\\overline{y}^{T} Q\\overline{w}+\\overline{w}^{T} Q^{T} Q\\overline{w} ) = (-2 Q^{T} \\overline{y}+ 2 Q^{T} Q \\overline{w}) = 0.$$\n",
    "\n",
    "Откуда немедленно получаем итоговую формулу для вектора весов: $$\\overline{w} = (Q^{T} Q)^{-1} Q^{T} \\overline{y}$$.\n",
    "\n",
    "\n",
    "Полученная формула имеет некоторые ограничения (подумайте какие). Также следует продумывать выбор базисных функций. Для получения *информационной матрицы* исходные данные требуют некоторого преобразования в зависимости от выбора базисных функций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70801eb",
   "metadata": {},
   "source": [
    "<font color = 'orange' size = 3>Пример 1 </font>\n",
    "\n",
    "Для начала используем встроенные возможности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0105482",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1) # задаём искусственные данные\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0aed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression #пробуем обучить стандартную модель регрессии из ЛР1 \n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e52f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x[:, np.newaxis]\n",
    "(X.shape,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167e876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testx = np.linspace(np.min(x),np.max(x),50)[:, np.newaxis]# по рисунку видим, что получили хрень\n",
    "plt.scatter(x, y, c ='red')\n",
    "plt.plot(testx,model.predict(testx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41b7ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.score(X,y)# позорище, а не модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.coef_,model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f87c50",
   "metadata": {},
   "source": [
    "А теперь добавим другой базис. Видно, что функция похожа на степенную. Используем полиномиальный базис.\n",
    "Класс [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) позволяет преобразовать ваши данные  и перейти от стандартного базиса ${x_1,x_2,...x_n}$ полиномиальному базису, который состоит из всех возможных функций $x_1^{k1}x_2^{k2}...x_n^{kn} : k1+k2+...kn<=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054248ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg=PolynomialFeatures(degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950805ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = poly_reg.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a68b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new #вместо одного x теперь целый вектор фич степени от 0 до 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "testx = np.linspace(np.min(x),np.max(x),50)[:, np.newaxis]\n",
    "plt.scatter(x, y, c ='red')\n",
    "plt.plot(testx,model.predict(poly_reg.fit_transform(testx)))\n",
    "# теперь для предсказания модели нужно передавать преобразованные в тот же базис данные\n",
    "# результат лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_new,y)# уже лучше!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c2ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(model.coef_,model.intercept_) # а вот и полученные веса для базисных функций. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c153a",
   "metadata": {},
   "source": [
    "**Упражнение** Подумайте, за что отвечает параметр fit_intercept и почему первый коэффициент равен 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b78384",
   "metadata": {},
   "source": [
    "<font color = 'orange' size = 3>Пример 2 </font>\n",
    "\n",
    "Теперь возьмём многомерные фичи. Используем рыбный датасет из файла Fish.csv и попробуем создать модель регрессии для предсказания веса (столбец weight).\n",
    "\n",
    "Попробуем использовать стандартную формулу для классической регрессии $f(\\overline{x})=w_{0} + \\sum\\limits_{j=1}^k w_{i} x_{i} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv(\"Fish.csv\") # загружаем данные\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe.drop([\"Species\",\"Weight\"],axis=1).values # выбираем то, что будет фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8830a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d97a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataframe[\"Weight\"].values # вес будем предсказывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True) # модель регрессии по умолчанию будет использовать стандартный базис\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfff2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb209e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eed529",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.coef_,model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4edd9",
   "metadata": {},
   "source": [
    "<font color = 'red' size = 5>Задание 2 </font>\n",
    "\n",
    "1. Изучите подробно описанные выше примеры.\n",
    "2. Реализуйте функции для работы регрессии по произвольному базису самостоятельно (используйте только базовые функции и numpy), используя формулы, полученные выше. Используйте матричные вычисления!\n",
    "3. Попрактикуйте модели и вашу и встроенную на разных искусственных данных.\n",
    "4. Постройте модели (встроенную и вашу) для прогнозирования веса рыбы из файла Fish.csv. Попробуйте различные базисы, различные комбинации фич и постарайтесь повысить точность прогноза.\n",
    "5. Попытайтесь предсказать [Цену дома (Med house val)](https://www.kaggle.com/datasets/shibumohapatra/house-price/data) используя вашу собственную и встроенные модели с различными базисами. \n",
    "6. Подумайте, как повлияет на модель использование слишком большого количества базисных фич.\n",
    "7. В примере выше мы тестируем полученную модель на тех же данных, что и обучаем. В целом это не очень хорошо. Разделите изначальные данные на обучающую и тестовую выборку и повторите вычисления точности.\n",
    "8. Попробуйте на тестовых или реальных данных повышать сложность модели. Например добавляя фичи всё большей и большей степени. Как это влияет на точность на обучющих и тестовых данных??\n",
    "\n",
    "\n",
    "Указание! Для оценки качества модели, в случае, если рисунок не возможен используйте [коэффициент детерминации](https://wiki.loginom.ru/articles/coefficient-of-determination.html): $$R^{2}=1-\\frac{(\\overline{y} -f(\\overline{x}))(\\overline{y} - f(\\overline{x}))^{T}}{(\\overline{y} - mean(\\overline{y}))(\\overline{y} - mean(\\overline{y}))^{T}},$$\n",
    "\n",
    "где $\\overline{y}$ - столбец обучающих меток, $f(\\overline{x})$ - функция предсказания применённая к вектор столбцу исходных признаков. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88a0de-95a7-475a-990f-2e1eb36a24e0",
   "metadata": {},
   "source": [
    "<font color = 'red' size = 5>Задание 3. </font>\n",
    "\n",
    "1. Используя полученные знания о линейной регрессии примените их на реальных данных.\n",
    "Применяйте различные методы, стройте валидационные кривые, подбирайте гиперпараметры, преобразуйте данные, для получения лучших по качеству эстиматоров.\n",
    "Данные о велосипедном трафике из файлов (fermont_bridge.csv) и (BycicleWeather.csv). В одном из файлов данные о количестве велосипедистов, проехавщих по мосту в виде временного ряда, а в другом данные о погоде в том же районе. Совместите и предскажите велотрафик.\n",
    "3. При работе используйте не только встроенные, но и свои собственные функции. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18866f",
   "metadata": {},
   "source": [
    "to be continued... 🧡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
